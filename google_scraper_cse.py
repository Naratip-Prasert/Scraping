import datetime
import json
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlencode
from time import sleep

import pandas as pd
import requests
import streamlit as st

# ======= Configuration =======
API_KEY = st.secrets["GOOGLE_API_KEY"]
CSE_ID = st.secrets["CUSTOM_SEARCH_ENGINE_ID"]
SEARCH_URL = "https://www.googleapis.com/customsearch/v1"

# ======= Helper Function =======
@st.cache_data
def fetch_google_results(query: str, extra_params: Optional[Dict[str, str]] = None) -> Tuple[List[Dict], Optional[str]]:
    if not query:
        return [], "Search query cannot be empty"
    if not API_KEY:
        return [], "Missing API Key"
    if not CSE_ID:
        return [], "Missing CSE ID"

    all_results: List[Dict] = []
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for start in range(1, 101, 10):
        params = {
            "key": API_KEY,
            "cx": CSE_ID,
            "q": query,
            "start": start,
            "num": 10,
            "gl": "th",
            "hl": "th",
        }
        if extra_params:
            params.update(extra_params)

        try:
            resp = requests.get(SEARCH_URL, params=params)
            resp.raise_for_status()
            data = resp.json()
        except requests.HTTPError as err:
            code = resp.status_code
            if code == 403:
                return [], "API quota exceeded or invalid key"
            return [], f"HTTP error: {err}"
        except Exception as err:
            return [], f"Request error: {err}"

        items = data.get("items")
        if not items:
            break

        for it in items:
            all_results.append({
                "title": it.get("title", ""),
                "link": it.get("link", ""),
                "date_scraped": now,
            })

    return all_results, None


def build_query_and_params(
    query: str,
    all_words: str,
    exact_phrase: str,
    any_words: str,
    none_words: str,
    num_from: str,
    num_to: str,
    site: str,
    filetype_ext: str,
    lr: str,
    cr: str,
    date_restrict: str,
    filter_facebook: bool,
    filter_x: bool
) -> Tuple[str, Dict[str, str]]:
    terms: List[str] = []
    if all_words:
        terms += all_words.split()
    if exact_phrase:
        terms.append(f'"{exact_phrase}"')
    if any_words:
        group = " OR ".join(any_words.split())
        terms.append(f'({group})')
    if none_words:
        terms += [f'-{w}' for w in none_words.split()]
    if num_from and num_to:
        terms.append(f'{num_from}..{num_to}')
    if site:
        terms.append(f'site:{site}')

    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° site filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Facebook / X
    site_filters = []
    if filter_facebook:
        site_filters.append("site:facebook.com")
    if filter_x:
        site_filters.append("site:x.com")
    if site_filters:
        terms.append("(" + " OR ".join(site_filters) + ")")

    q = query or " ".join(terms)
    if query and terms:
        q = f"{query} " + " ".join(terms)

    extras: Dict[str, str] = {}
    if filetype_ext:
        extras["fileType"] = filetype_ext
    if lr:
        extras["lr"] = lr
    if cr:
        extras["cr"] = cr
    if date_restrict:
        extras["dateRestrict"] = date_restrict

    return q, extras



# ======= Streamlit UI =======
def main():
    st.set_page_config(page_title="Google Multi-Keyword Scraper", layout="wide")
    st.title("üîé Google Advanced Multi-Keyword Scraper")

# ‚úÖ Checkbox ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Facebook ‡∏´‡∏£‡∏∑‡∏≠ X (‡∏Å‡πà‡∏≠‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô)
    col_fb, col_x = st.columns(2)
    filter_facebook = col_fb.checkbox("üîµ ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Facebook")
    filter_x = col_x.checkbox("‚ö´Ô∏è ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ X (Twitter)")

    st.markdown("üí° ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∏‡∏î ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ `Enter` ‡πÄ‡∏ä‡πà‡∏ô:\n```\n‡∏á‡∏≤‡∏ô\n‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡πÑ‡∏ó‡∏¢\nChulaScaping\n```")

    keyword_input = st.text_area("üìå ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô (‡πÉ‡∏™‡πà‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)", height=150)
    queries = [line.strip() for line in keyword_input.splitlines() if line.strip()]

# Advanced filters (optional)
    with st.expander("üîß ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (Optional Filters)"):
        all_words = st.text_input("‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ (All words)")
        exact_phrase = st.text_input("‡∏ï‡∏£‡∏á‡∏ß‡∏•‡∏µ‡∏ô‡∏µ‡πâ (Exact phrase)")
        any_words = st.text_input("‡∏Ñ‡∏≥‡πÉ‡∏î‡πÜ ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ (Any of these words)")
        none_words = st.text_input("‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ (None of these words)")
        col1, col2 = st.columns(2)
        num_from = col1.text_input("‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà", key="num_from")
        num_to = col2.text_input("‡∏ñ‡∏∂‡∏á", key="num_to")
        site = st.text_input("‡πÑ‡∏ã‡∏ï‡πå (site:)")
        filetype = st.selectbox("File type", ["", "pdf", "doc", "xls", "ppt"])
        lr = st.selectbox("Language", ["", "lang_th", "lang_en"])
        cr = st.selectbox("Country", ["", "countryTH", "countryUS"])
        date_restrict = st.selectbox("Date limit", ["", "d1", "w1", "m1"])

    if st.button("üöÄ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
        if not queries:
            st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î")
            return

        all_results = []
        with st.spinner(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(queries)} ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô..."):
            for idx, base_query in enumerate(queries):
                q, extras = build_query_and_params(
                base_query, all_words, exact_phrase, any_words, none_words,
                num_from, num_to, site, filetype, lr, cr, date_restrict,
                filter_facebook, filter_x
            )
                st.info(f"üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ({idx+1}/{len(queries)}): `{q}`")
                results, error = fetch_google_results(q, extras)
                if error:
                    st.warning(f"‚ùå {error}")
                else:
                    all_results.extend(results)
                sleep(1)

        if not all_results:
            st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏•‡∏¢")
            return

        df = pd.DataFrame(all_results).drop_duplicates(subset="link")
        st.success(f"üéâ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)")
        st.dataframe(df)
        df_display = df.copy()
        df_display["link"] = df_display["link"].apply(lambda url: f'<a href="{url}" target="_blank">{url}</a>')
        st.markdown("### üîó ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏î‡πâ")
        st.write("üìå ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
        st.write(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)

        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", data=csv, file_name="results.csv", mime="text/csv")

        towrite = pd.io.common.BytesIO()
        with pd.ExcelWriter(towrite, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="results")
        towrite.seek(0)
        st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel", data=towrite.read(), file_name="results.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


if __name__ == "__main__":
    main()
