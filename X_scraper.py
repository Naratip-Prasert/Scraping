import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
import io  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö buffer

st.title("üîê X Scraper")

with st.form("login_and_search"):
    st.subheader("üîê ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å X")
    username = st.text_input("Username ‡∏´‡∏£‡∏∑‡∏≠ Email", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    password = st.text_input("Password", type="password", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô")
    search_term = st.text_input("üîç ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô X", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà")
    submitted = st.form_submit_button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")

if submitted:
    st.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        driver.get("https://x.com/login")
        wait = WebDriverWait(driver, 20)

        username_input = wait.until(EC.presence_of_element_located((By.NAME, "text")))
        username_input.send_keys(username)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)

        password_input = wait.until(EC.presence_of_element_located((By.NAME, "password")))
        password_input.send_keys(password)
        password_input.send_keys(Keys.ENTER)
        time.sleep(5)

        search_box = wait.until(EC.presence_of_element_located((
            By.XPATH, '//input[@aria-label="Search"] | //input[@data-testid="SearchBox_Search_Input"]')))
        search_box.clear()
        search_box.send_keys(search_term)
        search_box.send_keys(Keys.ENTER)
        time.sleep(5)

        driver.get(driver.current_url + "&f=live")
        time.sleep(5)

        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        tweets = driver.find_elements(By.XPATH, '//article[@data-testid="tweet"]')[:30]
        data = []

        for t in tweets:
            try:
                text_el = t.find_element(By.XPATH, './/div[@data-testid="tweetText"]')
                text = text_el.text
            except:
                text = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)"
            try:
                name_el = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//span')
                user_name = name_el.text
            except:
                user_name = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ä‡∏∑‡πà‡∏≠)"

            try:
                id_el = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//a[contains(@href, "/")]')
                user_id = id_el.get_attribute("href").split("/")[-1]
            except:
                user_id = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ @id)"

            try:
                link_el = t.find_element(By.XPATH, './/a[contains(@href,"/status/")]')
                url = link_el.get_attribute("href")
            except:
                url = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå)"

            try:
                time_el = t.find_element(By.TAG_NAME, 'time')
                post_time = time_el.get_attribute("datetime")
            except:
                post_time = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå)"

            data.append({
                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå": user_name,
                "User ID (@...)": user_id,
                "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°": text[:150],
                "‡∏•‡∏¥‡∏á‡∏Å‡πå": url,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå": post_time,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà scraping": time.strftime("%Y-%m-%d %H:%M:%S")
            })

        df = pd.DataFrame(data)
        st.success(f"üéâ ‡πÄ‡∏à‡∏≠ {len(df)} ‡πÇ‡∏û‡∏™‡∏ï‡πå‡πÅ‡∏£‡∏Å")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå clickable
        df["‡∏•‡∏¥‡∏á‡∏Å‡πå"] = df["‡∏•‡∏¥‡∏á‡∏Å‡πå"].apply(lambda x: f'<a href="{x}" target="_blank">{x}</a>')
        st.write(df.to_html(escape=False), unsafe_allow_html=True)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV",
            data=csv_data,
            file_name='x_scraped_results.csv',
            mime='text/csv'
        )

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Results')
        excel_data = output.getvalue()

        st.download_button(
            label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel (.xlsx)",
            data=excel_data,
            file_name='x_scraped_results.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

    finally:
        driver.quit()
