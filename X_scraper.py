import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
import io
from datetime import date

st.set_page_config(page_title="X Scraper", page_icon="üîé", layout="wide")
st.markdown("<h1 style='text-align: center;'>üöÄ X Scraper</h1>", unsafe_allow_html=True)
st.markdown("### üîê Login and Advanced Search on [X](https://x.com) Automatically")

with st.form("login_and_search"):
    col1, col2 = st.columns(2)
    with col1:
        username = st.text_input("üë§ Username or Email", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    with col2:
        password = st.text_input("üîí Password", type="password", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô")

    search_term = st.text_input("üîç ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô X", placeholder="‡πÄ‡∏ä‡πà‡∏ô Chula")

    with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (Advanced Search)"):
        from_user = st.text_input("üìå ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÄ‡∏ä‡πà‡∏ô @chulatalk)")
        since_date = st.date_input("üìÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà", value=None)
        until_date = st.date_input("üìÖ ‡∏ñ‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", value=None)
        st.markdown("### üß† ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (Advanced Words)")
        all_words = st.text_input("üìò ‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥ (‡πÄ‡∏ä‡πà‡∏ô: cat dog)")
        exact_phrase = st.text_input("üîç ‡∏ß‡∏•‡∏µ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πä‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô: happy hour)")
        any_words = st.text_input("üìó ‡∏°‡∏µ‡∏Ñ‡∏≥‡πÉ‡∏î‡∏Ñ‡∏≥‡∏´‡∏ô‡∏∂‡πà‡∏á (‡πÄ‡∏ä‡πà‡∏ô: cat OR dog)")
        none_words = st.text_input("üìï ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ (‡πÄ‡∏ä‡πà‡∏ô: scam spam)")
        hashtags = st.text_input("üè∑Ô∏è Hashtag (‡πÄ‡∏ä‡πà‡∏ô: #TCAS)")

        only_images = st.checkbox("üñºÔ∏è ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û")
        only_videos = st.checkbox("üé• ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠")
        only_links = st.checkbox("üîó ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå")

    num_pages = st.slider("üìÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ", min_value=1, max_value=50, value=3)

    submitted = st.form_submit_button("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏¢!")

if submitted:
    st.info("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà...")

    search_query = search_term

    search_query = ""

    if all_words:
        search_query += f"{all_words} "
    if exact_phrase:
        search_query += f'"{exact_phrase}" '
    if any_words:
        search_query += ' OR '.join(any_words.split()) + " "
    if none_words:
        search_query += ' '.join(f"-{word}" for word in none_words.split()) + " "
    if hashtags:
        search_query += f"{hashtags} "

    if from_user:
        search_query += f" from:{from_user.replace('@', '')}"
    if since_date:
        search_query += f" since:{since_date.strftime('%Y-%m-%d')}"
    if until_date:
        search_query += f" until:{until_date.strftime('%Y-%m-%d')}"
    if only_images:
        search_query += " filter:images"
    if only_videos:
        search_query += " filter:videos"
    if only_links:
        search_query += " filter:links"

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        driver.get("https://x.com/login")
        wait = WebDriverWait(driver, 60)

        username_input = wait.until(EC.presence_of_element_located((By.NAME, "text")))
        username_input.send_keys(username)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)

        password_input = wait.until(EC.presence_of_element_located((By.NAME, "password")))
        password_input.send_keys(password)
        password_input.send_keys(Keys.ENTER)
        time.sleep(5)

        search_box = wait.until(EC.presence_of_element_located((
            By.XPATH, '//input[@aria-label="Search"] | //input[@data-testid="SearchBox_Search_Input"]')))
        search_box.clear()
        search_box.send_keys(search_query)
        search_box.send_keys(Keys.ENTER)
        time.sleep(5)

        driver.get(driver.current_url + "&f=live")
        time.sleep(5)

        last_count = 0
        for _ in range(num_pages):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            tweets = driver.find_elements(By.XPATH, '//article[@data-testid="tweet"]')
            if len(tweets) == last_count:
                break
            last_count = len(tweets)

        data = []
        for t in tweets:
            try:
                text = t.find_element(By.XPATH, './/div[@data-testid="tweetText"]').text
            except:
                text = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)"
            try:
                user_name = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//span').text
            except:
                user_name = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ä‡∏∑‡πà‡∏≠)"
            try:
                user_id = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//a[contains(@href, "/")]').get_attribute("href").split("/")[-1]
            except:
                user_id = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ @id)"
            try:
                url = t.find_element(By.XPATH, './/a[contains(@href,"/status/")]').get_attribute("href")
            except:
                url = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå)"
            try:
                post_time = t.find_element(By.TAG_NAME, 'time').get_attribute("datetime")
            except:
                post_time = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå)"

            data.append({
                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå": user_name,
                "User ID (@...)": user_id,
                "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°": text[:150],
                "‡∏•‡∏¥‡∏á‡∏Å‡πå": url,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå": post_time,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà scraping": time.strftime("%Y-%m-%d %H:%M:%S")
            })

        df = pd.DataFrame(data)
        st.markdown("---")
        st.markdown("## üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")

        st.success(f"üéâ ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: `{search_query}`")
        st.dataframe(df.drop(columns=["‡∏•‡∏¥‡∏á‡∏Å‡πå"]))

        st.markdown("### üîó ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡πÇ‡∏û‡∏™‡∏ï‡πå)")
        st.write(df[["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå", "User ID (@...)", "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°", "‡∏•‡∏¥‡∏á‡∏Å‡πå"]].to_html(escape=False), unsafe_allow_html=True)

        col_dl1, col_dl2 = st.columns(2)
        csv_data = df.to_csv(index=False).encode('utf-8')

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Results')
        output.seek(0)
        excel_data = output.read()

        with col_dl1:
            st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", data=csv_data, file_name="x_scraped_results.csv", mime="text/csv")
        with col_dl2:
            st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel (.xlsx)", data=excel_data, file_name="x_scraped_results.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

    finally:
        driver.quit()
