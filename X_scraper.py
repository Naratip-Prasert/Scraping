import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
import io

st.set_page_config(page_title="X Scraper", page_icon="üîé", layout="wide")
st.markdown("<h1 style='text-align: center;'>üöÄ X Scraper</h1>", unsafe_allow_html=True)
st.markdown("### üîê ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå [X](https://x.com) ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")

with st.form("login_and_search"):
    col1, col2 = st.columns(2)
    with col1:
        username = st.text_input("üë§ Username ‡∏´‡∏£‡∏∑‡∏≠ Email", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    with col2:
        password = st.text_input("üîí Password", type="password", placeholder="‡∏Å‡∏£‡∏≠‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô")

    search_term = st.text_input("üîç ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô X", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà")

    num_pages = st.slider("üìÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ (scroll)", min_value=1, max_value=50, value=3)

    submitted = st.form_submit_button("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏¢!")

if submitted:
    st.info("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà...")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        driver.get("https://x.com/login")
        wait = WebDriverWait(driver, 20)

        username_input = wait.until(EC.presence_of_element_located((By.NAME, "text")))
        username_input.send_keys(username)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)

        password_input = wait.until(EC.presence_of_element_located((By.NAME, "password")))
        password_input.send_keys(password)
        password_input.send_keys(Keys.ENTER)
        time.sleep(5)

        search_box = wait.until(EC.presence_of_element_located((
            By.XPATH, '//input[@aria-label="Search"] | //input[@data-testid="SearchBox_Search_Input"]')))
        search_box.clear()
        search_box.send_keys(search_term)
        search_box.send_keys(Keys.ENTER)
        time.sleep(5)

        driver.get(driver.current_url + "&f=live")
        time.sleep(5)

        for _ in range(num_pages):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        tweets = driver.find_elements(By.XPATH, '//article[@data-testid="tweet"]')[:30]
        data = []

        for t in tweets:
            try:
                text_el = t.find_element(By.XPATH, './/div[@data-testid="tweetText"]')
                text = text_el.text
            except:
                text = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)"
            try:
                name_el = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//span')
                user_name = name_el.text
            except:
                user_name = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ä‡∏∑‡πà‡∏≠)"

            try:
                id_el = t.find_element(By.XPATH, './/div[@data-testid="User-Name"]//a[contains(@href, "/")]')
                user_id = id_el.get_attribute("href").split("/")[-1]
            except:
                user_id = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ @id)"

            try:
                link_el = t.find_element(By.XPATH, './/a[contains(@href,"/status/")]')
                url = link_el.get_attribute("href")
            except:
                url = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå)"

            try:
                time_el = t.find_element(By.TAG_NAME, 'time')
                post_time = time_el.get_attribute("datetime")
            except:
                post_time = "(‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå)"

            data.append({
                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå": user_name,
                "User ID (@...)": user_id,
                "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°": text[:150],
                "‡∏•‡∏¥‡∏á‡∏Å‡πå": url,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå": post_time,
                "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà scraping": time.strftime("%Y-%m-%d %H:%M:%S")
            })

        df = pd.DataFrame(data)
        st.markdown("---")
        st.markdown("## üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")

        st.success(f"üéâ ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: `{search_term}`")
        st.dataframe(df.drop(columns=["‡∏•‡∏¥‡∏á‡∏Å‡πå"]))  # ‡∏ã‡πà‡∏≠‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏ß‡πâ‡∏à‡∏≤‡∏Å DataFrame ‡∏õ‡∏Å‡∏ï‡∏¥

        st.markdown("### üîó ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡πÇ‡∏û‡∏™‡∏ï‡πå)")
        st.write(df[["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå", "User ID (@...)", "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°", "‡∏•‡∏¥‡∏á‡∏Å‡πå"]].to_html(escape=False), unsafe_allow_html=True)

        # üéÅ ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        col_dl1, col_dl2 = st.columns(2)
        csv_data = df.to_csv(index=False).encode('utf-8')

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Results')
        output.seek(0)
        excel_data = output.read()
        with col_dl1:
            st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", data=csv_data, file_name="x_scraped_results.csv", mime="text/csv")
        with col_dl2:
            st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel (.xlsx)", data=excel_data, file_name="x_scraped_results.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    finally:
        driver.quit()
